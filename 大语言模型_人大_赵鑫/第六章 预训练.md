
# 预训练任务

# 模型训练参数设置
## batch size
1. batch较大：目标是为了提高训练的稳定性和吞吐量。 
   - 通常是百万token，M。例如Qwen-1.5是4M。 
3. 动态批次技术：batch size**先小后大**。在训练过程中逐渐增大批次大小，直到百万量级。
   - "先小"的目的：为了使用更少的数据量进行更高频次的反向传播，让loss尽快下降。 "后大"的目的：后期让模型训练更稳定，loss下降更稳定。
   - 例如GPT-3是从32K到3.2M。

<img width="600" height="500" alt="image" src="https://github.com/user-attachments/assets/68277eb9-b0d7-4478-afbb-3686e52709d9" />

## 学习率
1. 两个阶段：预热 和 衰减。
2. 预热阶段：目的是模型训练初期，参数是随机初始化，梯度较大，需要较小的学习率使得训练更稳定。
   - 占整个训练步骤(step)的0.1%到0.5%。
   - 采用**线性预热**，即学习率从一个很小的值，线性增加到设置的最大阈值。
   - **最大阈值**通常是5 * 10^-5 到 1 * 10^-4之间。
4. 衰减阶段：目的是达到最大阈值后学习率开始衰减，避免在局部最优点附近震荡。 一般衰减到最大阈值的10%。
   - 采用的技术：线性衰减，余弦衰减，平方根倒数衰减。
<img width="1320" height="492" alt="image" src="https://github.com/user-attachments/assets/42d45ae8-fac5-44d0-b925-8f6b65e16010" />

## 优化器
1. Adam及其变种AdamW
2. Adam
   - 使用**梯度的动量**作为参数更新方向。 历史更新步骤中的梯度加权平均值 作为 当前时刻的梯度。 目的是为了缓解样本随机性带来的损失震荡。
   - **自适应学习率**。使用梯度的加权二阶矩 对 梯度进行修改（类似使用标准差进行归一化）。 目的是为了防止梯度过小，导致模型难以优化。
   - 三个超参数，在LLM训练中的典型值：β1=0.9，β2=0.95，e=10^-8。
   - Adam优化器公式详见：https://github.com/tiantianheheaa/Machine-Learning-Deep-Learning/blob/main/%E4%BC%98%E5%8C%96%E5%99%A8Adam%E5%92%8CAdamW.md
3. Adafactor优化器，谷歌提出的，是Adam的一个变种。通过特殊设计可以节省显存，用于T5等模型的训练。

## 稳定训练技术
1. 背景：大模型训练经常遇到训练不稳定的情况。
2. 稳定训练的技术
  - 梯度裁剪：防止loss突增。将梯度限制在一个较小的区间。LLM的阈值通常设置为1。
  - 训练恢复：checkpoint。当训练异常，例如loss激增，从最近的checkpoint恢复。 跳过可能的有问题的数据。
  - 权重衰减：通过正则化技术来稳定训练。 例如AdamW采用权重衰减的方法，系数通常为0.1。
  - dropout：避免模型过拟合。 但是LLM中，考虑到训练数据和模型中的归一化结构，很少使用dropout。

# 高效模型训练技术
## 3D并行
## 零冗余优化器

## 激活重计算

## 混合精度训练


# 模型参数量和时空效率

## 模型参数量
1. 假设词表大小是V，模型是L层解码器，中间状态的维度是H，前馈层的中间状态维度是H'。
2. 输入emb层：全连接层，VH个参数。  V是词表大小，H是emb的维度。
3. 多头注意力层（由于需要堆叠L层，所以每层的输入输出的维度是相同的）：4个全连接层，4*H^2个参数。 Q、K、V的线性变换矩阵，都是H^2个参数。 将多头注意力的输出 拼接为 最终输出，需要一个全连接层H^2个参数。
4. 前馈层：3个全连接层，3*HH’个参数。 前两个全连接层将H映射到H'，最后一个全连接层将H'映射回H。
5. 归一化层：2个RMSNorm操作，2H个参数。 分别对多头注意力层和前馈层进行归一化。
6. 输出层：1个全连接层，VH个参数。 将维度H映射到词表大小V。
7. LLaMA-7B举例：2VH+H+L*(4H^2 + 3HH'+2H)， V=32000，L=32层，H=4096，H‘=11008。得到67 3841 5616，7B。 主要参数量是中间的L层堆叠，4个全连接（多头注意力）+3个全连接（前馈层）。
## 训练运算量
## 训练时间
1. 训练时间：浮点数运算、数据读写、多进程同步。主要耗时是浮点数运算，由GPU的浮点数运算能力决定。
2. 训练时间 = 运算量 / (GPU数量 * GPU 每秒浮点数运算)
3. GPU 每秒浮点数运算 通常是其理论浮点数运算的30%-70%。
4. 例如，LLaMA-65B，参数量P=6.5 * 10^10。词元数C=1.4 * 10^12。激活重计算技术，运算量=8CP。 2048张A100，理论计算值是每秒3.2 * 10^14次BF16浮点数运算，假设实际训练是2 * 10^14。 得到=8 * (6.5 * 10^10) * (1.4 * 10^12) / (2048 * 2 * 10^14) = 1.78 * 10^6秒=20.6天。

## 训练显存
