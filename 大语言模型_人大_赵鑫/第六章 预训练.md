
# 预训练任务

# 模型训练参数设置
## batch size
1. batch较大：目标是为了提高训练的稳定性和吞吐量。 
   - 通常是百万token，M。例如Qwen-1.5是4M。 
3. 动态批次技术：batch size**先小后大**。在训练过程中逐渐增大批次大小，直到百万量级。
   - "先小"的目的：为了使用更少的数据量进行更高频次的反向传播，让loss尽快下降。 "后大"的目的：后期让模型训练更稳定，loss下降更稳定。
   - 例如GPT-3是从32K到3.2M。

<img width="600" height="500" alt="image" src="https://github.com/user-attachments/assets/68277eb9-b0d7-4478-afbb-3686e52709d9" />

## 学习率
1. 两个阶段：预热 和 衰减。
2. 预热阶段：目的是模型训练初期，参数是随机初始化，梯度较大，需要较小的学习率使得训练更稳定。
   - 占整个训练步骤(step)的0.1%到0.5%。
   - 采用**线性预热**，即学习率从一个很小的值，线性增加到设置的最大阈值。
   - **最大阈值**通常是5 * 10^-5 到 1 * 10^-4之间。
4. 衰减阶段：目的是达到最大阈值后学习率开始衰减，避免在局部最优点附近震荡。 一般衰减到最大阈值的10%。
   - 采用的技术：线性衰减，余弦衰减，平方根倒数衰减。
<img width="1320" height="492" alt="image" src="https://github.com/user-attachments/assets/42d45ae8-fac5-44d0-b925-8f6b65e16010" />

## 优化器
1. Adam及其变种AdamW
   - Adam优化器公式详见：https://github.com/tiantianheheaa/Machine-Learning-Deep-Learning/blob/main/%E4%BC%98%E5%8C%96%E5%99%A8Adam%E5%92%8CAdamW.md
3. Adam
   - 使用**梯度的动量**作为参数更新方向。 历史更新步骤中的梯度加权平均值 作为 当前时刻的梯度。 目的是为了缓解样本随机性带来的损失震荡。
   - **自适应学习率**。使用梯度的加权二阶矩 对 梯度进行修改（类似使用标准差进行归一化）。 目的是为了防止梯度过小，导致模型难以优化。
   - 三个超参数，在LLM训练中的典型值：β1=0.9，β2=0.95，e=10^-8。
4. Adafactor优化器，谷歌提出的，是Adam的一个变种。通过特殊设计可以节省显存，用于T5等模型的训练。

## 稳定训练技术

# 高效模型训练技术
## 3D并行
## 零冗余优化器

## 激活重计算

## 混合精度训练


# 模型参数量和时空效率

## 模型参数量
## 训练运算量
## 训练时间

## 训练显存
