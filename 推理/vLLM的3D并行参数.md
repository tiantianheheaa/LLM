`tensor_parallel_size` 是 vLLM 推理框架中用于控制**模型张量并行（Tensor Parallelism）**程度的关键参数，其核心作用是将模型参数切分到多个 GPU 上并行计算，从而突破单 GPU 显存限制，支持更大规模模型的推理。以下是详细解释：

### **1. 参数含义**
- **定义**：`tensor_parallel_size` 表示参与张量并行的 GPU 数量。例如，设为 `4` 时，模型参数会被均匀切分到 4 个 GPU 上，每个 GPU 负责计算部分参数的运算。
- **作用范围**：仅在**多 GPU 环境**下生效（单 GPU 时自动设为 `1`，无并行效果）。
- **底层机制**：通过将矩阵乘法、注意力机制等操作拆分为多个子任务，并行执行后合并结果，实现计算加速和显存扩展。

### **2. 为什么需要张量并行？**
- **突破显存限制**：超大模型（如万亿参数）无法单卡运行，张量并行可将参数分散到多卡，降低单卡显存压力。
- **提升推理速度**：并行计算可缩短关键路径（如注意力层的计算时间），尤其适合高并发场景。
- **资源高效利用**：在多 GPU 服务器中，避免因单卡显存不足导致整体资源闲置。

### **3. 与 vLLM 中其他并行参数的关系**
vLLM 支持多种并行策略组合，`tensor_parallel_size` 通常与以下参数协同工作：
- **`pipeline_parallel_size`**：流水线并行，将模型层分配到不同 GPU，形成流水线执行。  
  - *示例*：`tensor_parallel_size=4` + `pipeline_parallel_size=2` = 8 卡并行（4 卡张量并行 × 2 卡流水线并行）。
- **`data_parallel_size`**：数据并行，将数据分片到多个 GPU，每个 GPU 运行完整模型副本。  
  - *注意*：vLLM 默认通过 `LLMEngine` 自动管理数据并行，通常无需直接设置此参数。

### **4. 参数设置建议**
- **单 GPU**：无需设置（自动为 `1`）。
- **多 GPU**：
  - **小规模集群（如 2-8 卡）**：优先使用张量并行（如 `tensor_parallel_size=4`），简化配置。
  - **大规模集群（如 16+ 卡）**：结合流水线并行（如 `tensor_parallel_size=8` + `pipeline_parallel_size=2`），平衡通信开销与计算效率。
- **性能调优**：
  - 增加 `tensor_parallel_size` 可提升吞吐量，但会引入 GPU 间通信开销（如 NVLink 带宽限制）。
  - 需通过实验确定最优值（如从 `2` 开始逐步增加，观察吞吐量和延迟变化）。

### **5. 示例场景**
假设在 8 卡 A100 服务器上运行 Qwen3-Max-Preview（万亿参数模型）：
- **配置 1**：`tensor_parallel_size=8`  
  - 模型参数完全切分到 8 卡，无流水线并行。  
  - 适合：模型层数较少但参数巨大的场景（如纯 Transformer 层）。
- **配置 2**：`tensor_parallel_size=4` + `pipeline_parallel_size=2`  
  - 4 卡负责张量并行计算单层，2 组卡形成流水线。  
  - 适合：模型层数多且参数分布均匀的场景（如混合专家模型 MoE）。

### **6. 注意事项**
- **通信开销**：张量并行需频繁同步中间结果（如 `All-Reduce` 操作），高 `tensor_parallel_size` 可能因通信延迟导致性能下降。
- **硬件兼容性**：需确保 GPU 间通过高速互联（如 NVLink、InfiniBand）连接，避免通信瓶颈。
- **模型支持**：并非所有模型都支持张量并行，需确认模型架构是否兼容（如 vLLM 默认支持大多数 Transformer 模型）。
