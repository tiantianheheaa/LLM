在Hugging Face的Transformers库中，`return_overflowing_tokens`和`return_length`是分词器（Tokenizer）的两个关键参数，用于处理长文本输入时的分块与长度信息记录。以下是它们的详细用法及技术原理：

### **一、`return_overflowing_tokens`：长文本分块控制**
#### **1. 核心功能**
- **作用**：当输入文本长度超过`max_length`时，默认会直接截断文本。若设置`return_overflowing_tokens=True`，分词器会将超长文本分割为多个块（chunks），并返回所有块（包括被截断的部分），而非仅保留前`max_length`个token。
- **典型场景**：处理长文档（如新闻、论文）时，需保留完整语义信息，避免关键内容被截断。

#### **2. 参数配合**
- **`max_length`**：定义每个块的最大长度（如`max_length=512`）。
- **`stride`**：控制相邻块的重叠长度（如`stride=128`），确保跨块信息连贯性。例如：
  ```python
  from transformers import AutoTokenizer
  tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
  text = "This is a long sentence... " * 1000  # 模拟长文本
  inputs = tokenizer(
      text,
      max_length=512,
      stride=128,
      return_overflowing_tokens=True,
      truncation=True  # 必须启用截断
  )
  print(len(inputs["input_ids"]))  # 输出块数量（如5个）
  ```

#### **3. 输出结构**
- **`input_ids`**：所有块的token ID列表（列表的列表）。
- **`overflow_to_sample_mapping`**：记录每个块属于原始文本的哪个样本（多文本输入时有用）。
- **示例输出**：
  ```python
  {
      "input_ids": [[101, 2023, ...], [2023, 3005, ...], ...],  # 多个块的token ID
      "overflow_to_sample_mapping": [0, 0, 1, 1]  # 块0-1属于样本0，块2-3属于样本1
  }
  ```

### **二、`return_length`：编码长度记录**
#### **1. 核心功能**
- **作用**：返回每个编码输入的实际长度（token数量），便于后续处理（如过滤短文本、动态填充）。
- **典型场景**：在批量处理不同长度文本时，需知道每个样本的实际长度以调整注意力掩码（attention mask）或填充策略。

#### **2. 参数配合**
- **`return_tensors`**：若设置为`"pt"`（PyTorch）或`"tf"`（TensorFlow），返回的`length`会是张量格式。
- **示例代码**：
  ```python
  inputs = tokenizer(
      ["Short text.", "A much longer sentence that exceeds the max length."],
      max_length=10,
      return_length=True,
      padding="max_length",  # 填充到max_length
      truncation=True
  )
  print(inputs["length"])  # 输出: [2, 10]（实际长度与填充后长度）
  ```

#### **3. 输出结构**
- **`length`**：每个输入样本的实际token数量（未填充前）。
- **与其他字段的关系**：
  - 若启用`padding`，`attention_mask`会标记填充位置（1为真实token，0为填充）。
  - `length`可用于动态调整模型输入（如RNN的序列长度参数）。

### **三、技术原理与显存优化**
#### **1. 分块机制与显存效率**
- **问题背景**：标准Transformer的上下文窗口固定（如512 tokens），超长文本需截断，导致信息丢失。
- **分块优势**：
  - **保留完整语义**：通过`stride`重叠分块，减少跨块信息断裂（如代词指代、逻辑衔接）。
  - **显存友好**：每个块独立处理，显存占用与`max_length`成正比，而非原始文本长度。
- **案例**：处理10K tokens的文本时：
  - **不分块**：直接截断到512 tokens，丢失95%信息。
  - **分块（stride=128）**：生成约75个块（每个512 tokens，重叠128 tokens），显存占用仅增加线性倍数。

#### **2. 长度记录与动态处理**
- **动态填充**：根据`length`字段，可对不同长度样本进行差异化填充（如仅填充到该批次最长样本长度，减少计算冗余）。
- **模型兼容性**：某些模型（如T5）需显式传入序列长度参数，`return_length`可自动生成此信息。

### **四、实际应用建议**
1. **长文本处理**：
   - 启用`return_overflowing_tokens=True` + `stride`，结合滑动窗口策略保留上下文。
   - 示例：问答系统中，将问题与文档分块后分别计算相似度，再聚合结果。
2. **批量处理优化**：
   - 通过`return_length`记录实际长度，实现动态批次处理（如`batch_encode_plus`的`pad_to_max_length=False`）。
3. **显存监控**：
   - 分块后，监控每个块的显存占用（如`torch.cuda.memory_allocated()`），避免单个块过大导致OOM。

### **五、参数对比表**
| 参数                | 作用                           | 典型场景                     | 显存影响               |
|---------------------|-------------------------------|----------------------------|-----------------------|
| `return_overflowing_tokens` | 长文本分块，返回所有溢出块       | 文档处理、问答系统           | 与块数量线性相关       |
| `return_length`      | 记录编码输入的实际长度           | 动态填充、序列长度敏感模型   | 忽略不计（仅存储整数） |
