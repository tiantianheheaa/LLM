### 自己总结
1. BERT只使用了Transformer架构的编码器encoder，没有使用解码器decoder。


---

好的，我们来对BERT（Bidirectional Encoder Representations from Transformers）的模型结构进行一次详尽的、逐层的深度解析。

BERT的核心思想是利用Transformer的Encoder（编码器）部分，通过大规模无监督语料的预训练，来学习深层的、双向的语言表示。其结构可以分为三个主要部分：**输入嵌入层（Input Embedding Layer）**、**多层Transformer编码器堆（Stack of Transformer Encoder Layers）**和**输出层（Output Layer）**。

---

### 一、 整体架构概览

BERT模型并非从零开始构建，而是完全借鉴了Transformer架构中的Encoder部分。与GPT（Generative Pre-trained Transformer）使用Decoder不同，BERT的双向特性使其能够同时利用一个词的上下文（左边和右边的词），这对于理解语言的深层语义至关重要。

BERT主要有两个版本，其核心区别在于层数和参数规模：
*   **BERT-Base**: 12层Transformer编码器，12个注意力头，隐藏层维度为768，总参数量约1.1亿。
*   **BERT-Large**: 24层Transformer编码器，16个注意力头，隐藏层维度为1024，总参数量约3.4亿。

下面，我们将从下到上，详细拆解每一层的结构。

### 二、 输入嵌入层 (Input Embedding Layer)

在进入Transformer编码器之前，原始文本需要被转换成模型可以处理的向量形式。BERT的输入嵌入是三种嵌入的叠加：

**输入向量 = Token Embedding + Segment Embedding + Position Embedding**

1.  **Token Embeddings (词元嵌入)**
    *   **作用**: 将文本序列中的每个词或子词（subword）映射为一个固定维度的向量。
    *   **WordPiece分词**: BERT使用WordPiece算法进行分词，它能有效处理未登录词（OOV）。例如，"running"可能被拆分为"run"和"##ning"，这两个子词共享"run"的语义信息。
    *   **特殊Token**:
        *   `[CLS]`: 每个输入序列的开头都会加上这个特殊词元。在分类任务中，这个词元对应的最终输出向量将作为整个序列的聚合表示，用于后续的分类预测。
        *   `[SEP]`: 用于分隔两个句子，或者放在单个句子的末尾。

2.  **Segment Embeddings (段嵌入)**
    *   **作用**: 区分输入序列中的不同句子。因为BERT的预训练任务之一（NSP）需要处理句子对。
    *   **实现**: 为第一个句子（Sentence A）的所有词元分配一个向量（通常全为0），为第二个句子（Sentence B）的所有词元分配另一个向量（通常全为1）。如果是单句任务，则所有词元的段嵌入都为0。

3.  **Position Embeddings (位置嵌入)**
    *   **作用**: 由于Transformer的自注意力机制本身不包含序列顺序信息，位置嵌入为模型提供了词元在序列中的位置信息。
    *   **实现**: 与原始Transformer使用固定的正弦/余弦函数不同，BERT使用**可学习的**位置嵌入。即模型在训练过程中会自己学习出一套位置向量，其维度与Token Embedding相同（Base版为768，Large版为1024）。模型最大支持512个词元的序列长度。

这三种嵌入向量逐元素相加，形成最终的输入向量矩阵，其形状为 `[batch_size, sequence_length, hidden_size]`，然后送入第一层Transformer编码器。

### 三、 Transformer编码器层 (Transformer Encoder Layer)

这是BERT的核心主体，由多个（12或24个）完全相同的编码器层堆叠而成。前一层的输出作为后一层的输入。**每一层编码器内部都包含两个核心子层和相应的连接技术**。

假设输入到某一层的矩阵为 `X`，其形状为 `[sequence_length, hidden_size]`。

#### 1. 多头自注意力机制 (Multi-Head Self-Attention)

这是BERT理解上下文关系的关键。它允许序列中的每个词元都“关注”到序列中所有其他词元，并计算出一个加权的表示。

*   **步骤详解**:
    1.  **线性投影**: 将输入 `X` 分别乘以三个可训练的权重矩阵 `W_Q`, `W_K`, `W_V`，得到查询（Query）、键（Key）、值（Value）三个矩阵。
        *   `Q = X * W_Q`
        *   `K = X * W_K`
        *   `V = X * W_V`
    2.  **缩放点积注意力计算**: 对每个词元，计算其Query向量与所有词元（包括自身）的Key向量的点积，并除以一个缩放因子 `sqrt(d_k)`（`d_k`是Key向量的维度，即`hidden_size / 注意力头数`），然后通过Softmax函数归一化，得到注意力权重。
        *   `Attention(Q, K, V) = softmax( (Q * K^T) / sqrt(d_k) ) * V`
    3.  **多头（Multi-Head）**: 为了让模型从不同“角度”关注上下文信息，BERT将输入和Q、K、V矩阵在维度上拆分为多个“头”（Head）。例如，BERT-Base有12个头，每个头的维度是 `768 / 12 = 64`。每个头独立执行上述的注意力计算。
    4.  **拼接与线性变换**: 将所有头计算出的注意力输出矩阵拼接起来，再乘以一个最终的权重矩阵 `W_O`，得到多头自注意力层的最终输出。
        *   `MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h) * W_O`

*   **残差连接与层归一化 (Add & Norm)**:
    *   将多头自注意力层的输出与该层的原始输入 `X` 进行**残差连接（Residual Connection）**，即 `X + MultiHeadOutput`。这能有效缓解深层网络的梯度消失问题。
    *   对相加后的结果进行**层归一化（Layer Normalization）**，使每个样本的特征分布稳定，加速训练。
    *   公式: `LayerNorm(X + MultiHeadAttention(X))`

#### 2. 前馈神经网络 (Feed-Forward Network, FFN)

在注意力层之后，数据会进入一个全连接的前馈神经网络，对每个位置的向量进行独立的非线性变换，以提取更复杂的特征。

*   **结构**: 这是一个两层的全连接网络。
    *   第一层：将维度从 `hidden_size`（如768）扩展到更高的维度（通常是 `4 * hidden_size`，如3072），并使用激活函数（原始论文使用ReLU，但后续如GELU也常被使用）。
    *   第二层：将维度从高维压缩回 `hidden_size`（如768）。
*   **公式**: `FFN(x) = max(0, x * W_1 + b_1) * W_2 + b_2`
    *   这里的 `x` 是上一个子层（Add & Norm）的输出。这个FFN对序列中的每个位置（每个词元）是独立应用的。

*   **再次残差连接与层归一化**:
    *   同样，FFN层的输出也会与其输入（即注意力子层的输出）进行残差连接，然后再次进行层归一化。
    *   公式: `LayerNorm(FFN_Input + FFN(FFN_Input))`

**一个完整的Transformer编码器层就由上述两个子层（多头自注意力和前馈网络）以及两次“Add & Norm”操作构成。** BERT-Base就是将这样的结构重复堆叠12次，BERT-Large则堆叠24次。

### 四、 预训练任务 (Pre-training Tasks)

BERT的强大能力来源于其两个独特的预训练任务，它们迫使模型学习语言的深层结构。

1.  **掩码语言模型 (Masked Language Model, MLM)**
    *   **目的**: 训练模型利用上下文（双向）来预测被掩盖的词。
    *   **过程**:
        1.  随机选择输入序列中15%的词元进行掩盖。
        2.  对于被选中的词元，采用以下策略：
            *   **80%** 的概率：替换为 `[MASK]` 词元。例如: `my dog is [MASK]`
            *   **10%** 的概率：替换为一个随机的词元。例如: `my dog is apple`。这迫使模型不能过分依赖 `[MASK]` 标记，必须结合上下文进行判断。
            *   **10%** 的概率：保持原词元不变。例如: `my dog is hairy`。这作为一种“偏置”，让模型倾向于保留词元本身的真实表征。
        3.  模型的任务是根据未被掩盖的上下文，准确预测出被掩盖位置的原始词元。

2.  **下一句预测 (Next Sentence Prediction, NSP)**
    *   **目的**: 训练模型理解句子之间的关系，这对于问答、自然语言推断等任务至关重要。
    *   **过程**:
        1.  输入是一对句子（A和B）。
        2.  其中50%的概率，句子B是句子A在原始语料中的真实下一句（标签为`IsNext`）。
        3.  另外50%的概率，句子B是从语料库中随机抽取的句子（标签为`NotNext`）。
        4.  模型需要预测这对句子是否是连续的，这是一个二分类任务。模型通常使用`[CLS]`词元的最终输出向量来进行预测。
    *   *注：后续研究（如RoBERTa）表明，NSP任务的效果不如预期，甚至可能损害性能，因此在一些后续模型中被移除或改进。*

### 五、 输出层 (Output Layer)与微调 (Fine-tuning)

预训练完成后，BERT模型可以通过在特定任务上进行“微调”来适应各种下游任务。输出层的结构会根据任务类型而变化：

*   **句对分类任务 (如MNLI)**: 使用`[CLS]`词元的最终输出向量，送入一个Softmax分类器，预测类别。
*   **单句分类任务 (如情感分析)**: 与句对分类类似，使用`[CLS]`词元的输出进行分类。
*   **问答任务 (如SQuAD)**: 将问题和上下文拼接后输入BERT。模型会为每个词元输出一个向量，通过两个额外的全连接层，分别预测答案的起始位置和结束位置。
*   **单词标注任务 (如命名实体识别)**: 使用每个输入词元对应的最终输出向量，送入一个分类器，为每个词元预测一个标签（如人名、地名、组织名等）。

### 总结

BERT的结构细节可以概括为：

*   **基础**: 纯Transformer Encoder堆叠（12或24层）。
*   **输入**: Token、Segment、Position三种嵌入的叠加。
*   **核心层**: 每层包含“多头自注意力”和“前馈网络”两个子层，并由“残差连接”和“层归一化”包裹。
*   **训练**: 通过“掩码语言模型（MLM）”和“下一句预测（NSP）”两个任务进行无监督预训练，学习深度的双向语言表示。
*   **应用**: 预训练模型通过简单的输出层调整，即可在多种NLP任务上通过微调取得卓越效果。

这种精巧而强大的设计，使得BERT成为了自然语言处理领域的一个里程碑，并开启了“预训练-微调”范式的新时代。
